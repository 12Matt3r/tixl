#!/usr/bin/env python3
"""
TiXL Vulnerability Aggregator
Aggregates and analyzes security scan results from multiple tools
"""

import json
import os
import sys
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from dataclasses import dataclass
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class Vulnerability:
    """Represents a security vulnerability"""
    id: str
    title: str
    description: str
    severity: str
    cvss_score: Optional[float]
    tool: str
    category: str
    file_path: Optional[str]
    line_number: Optional[int]
    cve_id: Optional[str]
    cwe_id: Optional[str]
    package_name: Optional[str]
    package_version: Optional[str]
    fixed_version: Optional[str]
    is_false_positive: bool = False
    status: str = "open"  # open, dismissed, fixed
    created_date: Optional[str] = None
    last_seen_date: Optional[str] = None

class VulnerabilityAggregator:
    """Aggregates vulnerabilities from multiple security scanning tools"""
    
    def __init__(self, severity_threshold: str = "medium"):
        self.severity_threshold = severity_threshold.lower()
        self.severity_levels = {
            'critical': 4,
            'high': 3,
            'medium': 2,
            'low': 1,
            'info': 0
        }
        
        self.vulnerabilities = []
        self.aggregated_data = {
            'scan_info': {
                'timestamp': datetime.utcnow().isoformat(),
                'severity_threshold': severity_threshold
            },
            'summary': {},
            'vulnerabilities_by_tool': {},
            'vulnerabilities_by_severity': {},
            'vulnerabilities_by_category': {}
        }
    
    def load_scan_results(self, input_dir: Path) -> None:
        """Load results from all security scanning tools"""
        logger.info(f"Loading scan results from {input_dir}")
        
        # Load CodeQL results
        self._load_codeql_results(input_dir)
        
        # Load SonarQube results
        self._load_sonarqube_results(input_dir)
        
        # Load Semgrep results
        self._load_semgrep_results(input_dir)
        
        # Load NuGet/SCA results
        self._load_sca_results(input_dir)
        
        # Load secret scanning results
        self._load_secret_scanning_results(input_dir)
        
        # Load container security results
        self._load_container_security_results(input_dir)
        
        # Load IaC security results
        self._load_iac_security_results(input_dir)
        
        logger.info(f"Loaded {len(self.vulnerabilities)} total vulnerabilities")
    
    def _load_codeql_results(self, input_dir: Path) -> None:
        """Load CodeQL SARIF results"""
        codeql_dir = input_dir / "codeql-results-"
        if not codeql_dir.exists():
            return
            
        for sarif_file in codeql_dir.rglob("*.sarif"):
            try:
                with open(sarif_file, 'r', encoding='utf-8') as f:
                    sarif_data = json.load(f)
                
                for run in sarif_data.get('runs', []):
                    for result in run.get('results', []):
                        if result.get('level') in ['error', 'warning', 'note']:
                            vuln = self._parse_codeql_result(result)
                            if vuln:
                                self.vulnerabilities.append(vuln)
                                
                logger.info(f"Loaded CodeQL results from {sarif_file}")
            except Exception as e:
                logger.error(f"Error loading CodeQL results from {sarif_file}: {e}")
    
    def _parse_codeql_result(self, result: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse a CodeQL SARIF result"""
        try:
            # Extract basic information
            vuln_id = result.get('ruleId', 'codeql-unknown')
            title = result.get('message', {}).get('text', 'CodeQL finding')
            
            # Determine severity
            level = result.get('level', 'note')
            severity_map = {'error': 'high', 'warning': 'medium', 'note': 'low'}
            severity = severity_map.get(level, 'low')
            
            # Extract location information
            file_path = None
            line_number = None
            
            if result.get('locations'):
                location = result['locations'][0]
                if 'physicalLocation' in location:
                    pl = location['physicalLocation']
                    file_path = pl.get('artifactLocation', {}).get('uri')
                    line_number = pl.get('region', {}).get('startLine')
            
            # Extract additional metadata
            rule = result.get('rule', {})
            cwe_id = None
            if 'properties' in rule:
                props = rule['properties']
                cwe_id = props.get('problem.severity')
            
            return Vulnerability(
                id=f"codeql-{vuln_id}",
                title=title,
                description=title,
                severity=severity,
                cvss_score=None,
                tool="CodeQL",
                category="SAST",
                file_path=file_path,
                line_number=line_number,
                cve_id=None,
                cwe_id=cwe_id,
                package_name=None,
                package_version=None,
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing CodeQL result: {e}")
            return None
    
    def _load_semgrep_results(self, input_dir: Path) -> None:
        """Load Semgrep SARIF results"""
        semgrep_dirs = list(input_dir.glob("semgrep-results-*"))
        for semgrep_dir in semgrep_dirs:
            for sarif_file in semgrep_dir.rglob("*.sarif"):
                try:
                    with open(sarif_file, 'r', encoding='utf-8') as f:
                        sarif_data = json.load(f)
                    
                    for run in sarif_data.get('runs', []):
                        for result in run.get('results', []):
                            vuln = self._parse_semgrep_result(result)
                            if vuln:
                                self.vulnerabilities.append(vuln)
                    
                    logger.info(f"Loaded Semgrep results from {sarif_file}")
                except Exception as e:
                    logger.error(f"Error loading Semgrep results from {sarif_file}: {e}")
    
    def _parse_semgrep_result(self, result: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse a Semgrep SARIF result"""
        try:
            # Extract basic information
            rule_id = result.get('ruleId', 'semgrep-unknown')
            title = result.get('message', {}).get('text', 'Semgrep finding')
            
            # Extract metadata
            severity = result.get('level', 'low')
            if severity == 'ERROR':
                severity = 'high'
            elif severity == 'WARNING':
                severity = 'medium'
            else:
                severity = 'low'
            
            # Extract location information
            file_path = None
            line_number = None
            
            if result.get('locations'):
                location = result['locations'][0]
                if 'physicalLocation' in location:
                    pl = location['physicalLocation']
                    file_path = pl.get('artifactLocation', {}).get('uri')
                    line_number = pl.get('region', {}).get('startLine')
            
            return Vulnerability(
                id=f"semgrep-{rule_id}",
                title=title,
                description=title,
                severity=severity,
                cvss_score=None,
                tool="Semgrep",
                category="SAST",
                file_path=file_path,
                line_number=line_number,
                cve_id=None,
                cwe_id=rule_id,
                package_name=None,
                package_version=None,
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing Semgrep result: {e}")
            return None
    
    def _load_sca_results(self, input_dir: Path) -> None:
        """Load SCA (dependency vulnerability) results"""
        sca_dirs = list(input_dir.glob("sca-results-*"))
        for sca_dir in sca_dirs:
            # Load Grype results
            grype_file = sca_dir / "grype-results.json"
            if grype_file.exists():
                try:
                    with open(grype_file, 'r', encoding='utf-8') as f:
                        grype_data = json.load(f)
                    
                    for match in grype_data.get('matches', []):
                        vuln = self._parse_grype_result(match)
                        if vuln:
                            self.vulnerabilities.append(vuln)
                    
                    logger.info(f"Loaded {len(grype_data.get('matches', []))} Grype findings")
                except Exception as e:
                    logger.error(f"Error loading Grype results from {grype_file}: {e}")
            
            # Load dotnet-retire results
            retire_file = sca_dir / "retire-results.json"
            if retire_file.exists():
                try:
                    with open(retire_file, 'r', encoding='utf-8') as f:
                        retire_data = json.load(f)
                    
                    for finding in retire_data:
                        vuln = self._parse_retire_result(finding)
                        if vuln:
                            self.vulnerabilities.append(vuln)
                    
                    logger.info(f"Loaded {len(retire_data)} dotnet-retire findings")
                except Exception as e:
                    logger.error(f"Error loading dotnet-retire results from {retire_file}: {e}")
    
    def _parse_grype_result(self, match: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse a Grype vulnerability result"""
        try:
            vuln = match.get('vulnerability', {})
            artifact = match.get('artifact', {})
            
            vuln_id = vuln.get('id', 'grype-unknown')
            severity = vuln.get('severity', 'low')
            if severity.upper() not in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                severity = 'low'
            else:
                severity = severity.lower()
            
            return Vulnerability(
                id=f"grype-{vuln_id}",
                title=vuln.get('description', 'Vulnerable dependency'),
                description=vuln.get('description', ''),
                severity=severity,
                cvss_score=vuln.get('cvss', [{}])[0].get('baseScore'),
                tool="Grype",
                category="SCA",
                file_path=None,
                line_number=None,
                cve_id=vuln_id,
                cwe_id=None,
                package_name=artifact.get('name'),
                package_version=artifact.get('version'),
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing Grype result: {e}")
            return None
    
    def _parse_retire_result(self, finding: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse a dotnet-retire result"""
        try:
            # dotnet-retire typically returns file paths and recommendations
            file_path = finding.get('file', 'unknown')
            recommendation = finding.get('recommendation', 'Update to latest version')
            
            return Vulnerability(
                id=f"retire-{hash(file_path)}",
                title="Known vulnerable .NET package",
                description=recommendation,
                severity="high",  # dotnet-retire findings are typically high severity
                cvss_score=None,
                tool="dotnet-retire",
                category="SCA",
                file_path=file_path,
                line_number=None,
                cve_id=None,
                cwe_id=None,
                package_name=finding.get('packageName'),
                package_version=finding.get('version'),
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing dotnet-retire result: {e}")
            return None
    
    def _load_sonarqube_results(self, input_dir: Path) -> None:
        """Load SonarQube results"""
        sonar_dirs = list(input_dir.glob("sonarqube-results-*"))
        for sonar_dir in sonar_dirs:
            # SonarQube results are typically in specific directories
            issues_file = sonar_dir / ".sonarqube" / "out" / "sonar" / "report-task.txt"
            if issues_file.exists():
                # SonarQube typically provides HTML reports
                logger.info("SonarQube results found (HTML format)")
    
    def _load_secret_scanning_results(self, input_dir: Path) -> None:
        """Load secret scanning results"""
        secret_dirs = list(input_dir.glob("secret-scanning-*"))
        for secret_dir in secret_dirs:
            for result_file in secret_dir.rglob("*.json"):
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        secret_data = json.load(f)
                    
                    # Process TruffleHog results
                    if 'SourceMetadata' in secret_data:
                        vuln = self._parse_trufflehog_result(secret_data)
                        if vuln:
                            self.vulnerabilities.append(vuln)
                    
                    logger.info(f"Loaded secret scanning results from {result_file}")
                except Exception as e:
                    logger.error(f"Error loading secret scanning results from {result_file}: {e}")
    
    def _parse_trufflehog_result(self, finding: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse a TruffleHog secret finding"""
        try:
            source_metadata = finding.get('SourceMetadata', {})
            git = source_metadata.get('Data', {}).get('Filesystem', {})
            
            file_path = git.get('file')
            line_number = git.get('line')
            secret_type = finding.get('DetectorName', 'Unknown secret')
            redacted = finding.get('Redacted', 'REDACTED')
            
            return Vulnerability(
                id=f"secret-{hash(str(finding))}",
                title=f"Secret detected: {secret_type}",
                description=f"Potential secret detected: {redacted}",
                severity="high",  # All secrets are high severity
                cvss_score=8.0,  # Reasonable default for secrets
                tool="TruffleHog",
                category="Secret Scanning",
                file_path=file_path,
                line_number=line_number,
                cve_id=None,
                cwe_id="CWE-798",  # Use of hardcoded credentials
                package_name=None,
                package_version=None,
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing TruffleHog result: {e}")
            return None
    
    def _load_container_security_results(self, input_dir: Path) -> None:
        """Load container security results"""
        container_dirs = list(input_dir.glob("container-security-*"))
        for container_dir in container_dirs:
            for sarif_file in container_dir.rglob("*.sarif"):
                try:
                    with open(sarif_file, 'r', encoding='utf-8') as f:
                        sarif_data = json.load(f)
                    
                    for run in sarif_data.get('runs', []):
                        for result in run.get('results', []):
                            vuln = self._parse_container_result(result)
                            if vuln:
                                self.vulnerabilities.append(vuln)
                    
                    logger.info(f"Loaded container security results from {sarif_file}")
                except Exception as e:
                    logger.error(f"Error loading container security results from {sarif_file}: {e}")
    
    def _parse_container_result(self, result: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse a container security result"""
        try:
            vuln_id = result.get('ruleId', 'container-unknown')
            title = result.get('message', {}).get('text', 'Container security finding')
            severity = 'high'  # Default to high for container issues
            
            return Vulnerability(
                id=f"container-{vuln_id}",
                title=title,
                description=title,
                severity=severity,
                cvss_score=None,
                tool="Trivy",
                category="Container Security",
                file_path=None,
                line_number=None,
                cve_id=None,
                cwe_id=None,
                package_name=None,
                package_version=None,
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing container security result: {e}")
            return None
    
    def _load_iac_security_results(self, input_dir: Path) -> None:
        """Load Infrastructure as Code security results"""
        iac_dirs = list(input_dir.glob("iac-security-*"))
        for iac_dir in iac_dirs:
            for sarif_file in iac_dir.rglob("*.sarif"):
                try:
                    with open(sarif_file, 'r', encoding='utf-8') as f:
                        sarif_data = json.load(f)
                    
                    for run in sarif_data.get('runs', []):
                        for result in run.get('results', []):
                            vuln = self._parse_iac_result(result)
                            if vuln:
                                self.vulnerabilities.append(vuln)
                    
                    logger.info(f"Loaded IaC security results from {sarif_file}")
                except Exception as e:
                    logger.error(f"Error loading IaC security results from {sarif_file}: {e}")
    
    def _parse_iac_result(self, result: Dict[str, Any]) -> Optional[Vulnerability]:
        """Parse an Infrastructure as Code security result"""
        try:
            vuln_id = result.get('ruleId', 'iac-unknown')
            title = result.get('message', {}).get('text', 'IaC security finding')
            severity = 'medium'  # Default to medium for IaC issues
            
            return Vulnerability(
                id=f"iac-{vuln_id}",
                title=title,
                description=title,
                severity=severity,
                cvss_score=None,
                tool="Checkov",
                category="IaC Security",
                file_path=None,
                line_number=None,
                cve_id=None,
                cwe_id=None,
                package_name=None,
                package_version=None,
                fixed_version=None,
                created_date=datetime.utcnow().isoformat(),
                last_seen_date=datetime.utcnow().isoformat()
            )
        except Exception as e:
            logger.error(f"Error parsing IaC security result: {e}")
            return None
    
    def analyze_vulnerabilities(self) -> None:
        """Analyze and categorize vulnerabilities"""
        logger.info("Analyzing vulnerabilities...")
        
        # Aggregate data
        by_tool = defaultdict(list)
        by_severity = defaultdict(list)
        by_category = defaultdict(list)
        
        for vuln in self.vulnerabilities:
            by_tool[vuln.tool].append(vuln)
            by_severity[vuln.severity].append(vuln)
            by_category[vuln.category].append(vuln)
        
        # Convert to serializable format
        self.aggregated_data['vulnerabilities_by_tool'] = {
            tool: [self._vulnerability_to_dict(v) for v in vulns]
            for tool, vulns in by_tool.items()
        }
        
        self.aggregated_data['vulnerabilities_by_severity'] = {
            severity: [self._vulnerability_to_dict(v) for v in vulns]
            for severity, vulns in by_severity.items()
        }
        
        self.aggregated_data['vulnerabilities_by_category'] = {
            category: [self._vulnerability_to_dict(v) for v in vulns]
            for category, vulns in by_category.items()
        }
        
        # Generate summary statistics
        self.aggregated_data['summary'] = {
            'total_vulnerabilities': len(self.vulnerabilities),
            'by_severity': {k: len(v) for k, v in by_severity.items()},
            'by_tool': {k: len(v) for k, v in by_tool.items()},
            'by_category': {k: len(v) for k, v in by_category.items()},
            'high_severity_or_above': len([v for v in self.vulnerabilities 
                                         if self.severity_levels.get(v.severity, 0) >= self.severity_levels.get('high', 3)])
        }
        
        logger.info("Vulnerability analysis complete")
    
    def _vulnerability_to_dict(self, vuln: Vulnerability) -> Dict[str, Any]:
        """Convert vulnerability to dictionary"""
        return {
            'id': vuln.id,
            'title': vuln.title,
            'description': vuln.description,
            'severity': vuln.severity,
            'cvss_score': vuln.cvss_score,
            'tool': vuln.tool,
            'category': vuln.category,
            'file_path': vuln.file_path,
            'line_number': vuln.line_number,
            'cve_id': vuln.cve_id,
            'cwe_id': vuln.cwe_id,
            'package_name': vuln.package_name,
            'package_version': vuln.package_version,
            'fixed_version': vuln.fixed_version,
            'is_false_positive': vuln.is_false_positive,
            'status': vuln.status,
            'created_date': vuln.created_date,
            'last_seen_date': vuln.last_seen_date
        }
    
    def filter_by_severity(self) -> None:
        """Filter vulnerabilities by severity threshold"""
        if self.severity_threshold not in self.severity_levels:
            logger.warning(f"Unknown severity threshold: {self.severity_threshold}")
            return
        
        threshold_level = self.severity_levels[self.severity_threshold]
        
        filtered_vulns = [
            v for v in self.vulnerabilities
            if self.severity_levels.get(v.severity, 0) >= threshold_level
        ]
        
        logger.info(f"Filtered {len(self.vulnerabilities)} vulnerabilities to {len(filtered_vulns)} based on severity threshold: {self.severity_threshold}")
        self.vulnerabilities = filtered_vulns
    
    def save_results(self, output_dir: Path) -> None:
        """Save aggregated results to output directory"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save raw vulnerability data
        with open(output_dir / 'vulnerabilities.json', 'w', encoding='utf-8') as f:
            json.dump([self._vulnerability_to_dict(v) for v in self.vulnerabilities], f, indent=2)
        
        # Save aggregated analysis
        with open(output_dir / 'analysis.json', 'w', encoding='utf-8') as f:
            json.dump(self.aggregated_data, f, indent=2)
        
        # Save summary report
        self._generate_summary_report(output_dir)
        
        logger.info(f"Results saved to {output_dir}")
    
    def _generate_summary_report(self, output_dir: Path) -> None:
        """Generate a summary report"""
        summary = self.aggregated_data['summary']
        
        report_lines = [
            "# TiXL Security Vulnerability Summary",
            "",
            f"**Generated**: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}",
            f"**Severity Threshold**: {self.severity_threshold}",
            "",
            "## Overview",
            f"- **Total Vulnerabilities**: {summary['total_vulnerabilities']}",
            f"- **High Severity or Above**: {summary['high_severity_or_above']}",
            "",
            "## By Severity",
        ]
        
        for severity in ['critical', 'high', 'medium', 'low']:
            count = summary['by_severity'].get(severity, 0)
            emoji = {'critical': 'ðŸ”´', 'high': 'ðŸŸ ', 'medium': 'ðŸŸ¡', 'low': 'ðŸŸ¢'}.get(severity, 'âšª')
            report_lines.append(f"- {emoji} **{severity.title()}**: {count}")
        
        report_lines.extend([
            "",
            "## By Category",
        ])
        
        for category, count in summary['by_category'].items():
            report_lines.append(f"- **{category}**: {count}")
        
        report_lines.extend([
            "",
            "## By Tool",
        ])
        
        for tool, count in summary['by_tool'].items():
            report_lines.append(f"- **{tool}**: {count}")
        
        # Top 10 issues
        if self.vulnerabilities:
            report_lines.extend([
                "",
                "## Top Priority Issues",
                ""
            ])
            
            sorted_vulns = sorted(
                self.vulnerabilities,
                key=lambda v: (self.severity_levels.get(v.severity, 0), v.title),
                reverse=True
            )[:10]
            
            for i, vuln in enumerate(sorted_vulns, 1):
                report_lines.append(f"{i}. **{vuln.title}**")
                report_lines.append(f"   - Severity: {vuln.severity}")
                report_lines.append(f"   - Tool: {vuln.tool}")
                report_lines.append(f"   - Category: {vuln.category}")
                if vuln.file_path:
                    report_lines.append(f"   - Location: {vuln.file_path}:{vuln.line_number or 'N/A'}")
                report_lines.append("")
        
        # Save report
        with open(output_dir / 'summary.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

def main():
    parser = argparse.ArgumentParser(description='TiXL Vulnerability Aggregator')
    parser.add_argument('--input-dir', type=Path, required=True, 
                       help='Directory containing security scan results')
    parser.add_argument('--output-dir', type=Path, required=True,
                       help='Directory to save aggregated results')
    parser.add_argument('--severity-threshold', type=str, default='medium',
                       choices=['low', 'medium', 'high', 'critical'],
                       help='Minimum severity to include in results')
    parser.add_argument('--generate-report', action='store_true',
                       help='Generate summary reports')
    
    args = parser.parse_args()
    
    # Initialize aggregator
    aggregator = VulnerabilityAggregator(severity_threshold=args.severity_threshold)
    
    try:
        # Load all scan results
        aggregator.load_scan_results(args.input_dir)
        
        # Filter by severity if requested
        if args.severity_threshold:
            aggregator.filter_by_severity()
        
        # Analyze vulnerabilities
        aggregator.analyze_vulnerabilities()
        
        # Save results
        aggregator.save_results(args.output_dir)
        
        logger.info("Vulnerability aggregation completed successfully")
        
        # Print summary to stdout for CI consumption
        summary = aggregator.aggregated_data['summary']
        print(f"VULNERABILITY_SUMMARY_TOTAL={summary['total_vulnerabilities']}")
        print(f"VULNERABILITY_SUMMARY_HIGH_OR_ABOVE={summary['high_severity_or_above']}")
        
        for severity, count in summary['by_severity'].items():
            print(f"VULNERABILITY_COUNT_{severity.upper()}={count}")
            
        # Determine exit code based on findings
        if summary['high_severity_or_above'] > 0:
            logger.warning(f"Found {summary['high_severity_or_above']} high-severity or above vulnerabilities")
            sys.exit(1)
        else:
            logger.info("No high-severity vulnerabilities found")
            sys.exit(0)
            
    except Exception as e:
        logger.error(f"Error during vulnerability aggregation: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()